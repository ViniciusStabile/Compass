# Resumo da Sprint 5 ‚Äì Big Data com Spark e Analytics na AWS

Nesta Sprint 5, aprofundei meus conhecimentos em **processamento distribu√≠do com Apache Spark** e em **solu√ß√µes de analytics na AWS**, fortalecendo compet√™ncias essenciais para atuar em projetos de engenharia de dados em larga escala. Estudei a fundo o uso do **PySpark** para manipula√ß√£o de dados em cluster e explorei servi√ßos como **Amazon Athena**, **analytics serverless** e os fundamentos do ecossistema de dados da AWS.

---

## Forma√ß√£o Spark com PySpark: O Curso Completo

- Fundamentos do Apache Spark e arquitetura de cluster.
- Compara√ß√µes entre RDDs e DataFrames com foco em desempenho.
- Manipula√ß√£o e transforma√ß√£o de dados com PySpark.
- Consultas com **Spark SQL**, integra√ß√£o com arquivos CSV, JSON e Parquet.
- Desenvolvimento de aplica√ß√µes anal√≠ticas e boas pr√°ticas de otimiza√ß√£o de jobs Spark.
- Explora√ß√£o de t√≥picos avan√ßados, como **tuning de performance**, **persist√™ncia de dados** e **particionamento**.

---

## AWS Skill Builder: Fundamentals of Analytics on AWS ‚Äì Part 1

- Introdu√ß√£o aos conceitos de analytics na nuvem e aos **5 Vs do Big Data**.
- Mapeamento dos servi√ßos de analytics da AWS para desafios reais de neg√≥cio.
- Vis√£o geral sobre coleta, armazenamento, processamento e visualiza√ß√£o de dados.
- Base para continuar a trilha de aprendizado em analytics com a AWS.

---

##  AWS Skill Builder: Introduction to Amazon Athena

- Vis√£o geral do **Amazon Athena**, servi√ßo serverless de consulta SQL sobre dados no S3.
- Demonstra√ß√£o pr√°tica da cria√ß√£o de banco de dados e execu√ß√£o de queries diretamente na AWS.
- Aplica√ß√µes pr√°ticas em an√°lises explorat√≥rias e integra√ß√£o com pipelines de dados.

## AWS Skill Builder: Serverless Analytics

- Abordagem moderna de analytics usando servi√ßos serverless para escalabilidade e efici√™ncia.
- Import√¢ncia da an√°lise de dados em tempo real e em m√∫ltiplos formatos.
- Demonstra√ß√µes pr√°ticas do uso de servi√ßos como **Amazon S3, Glue, Athena e QuickSight**.

#  Desafio

- O arquivo desenvolvido e utilizado para a realiza√ß√£o do desafio desta sprint est√° dispon√≠vel na pasta Desafio, e a documenta√ß√£o completa pode ser consultada em seu respectivo `README.md`:
  - üìÇ [Pasta Desafio](./Desafio/)
  - üìÑ [README.md do Desafio](./Desafio/README.md)

# Exerc√≠cios

Nesta Sprint, realizei o exerc√≠cio **Contador de Palavras com Apache Spark**, onde recebemos a imagem Docker `jupyter/all-spark-notebook` para realizar a atividade dentro de um container. 

Tambem realizei o exercicio do **TMDB** onde aprendemos a fazer uma requisi√ß√£o na api do **TMDB**

##  Etapas Realizadas exercicio 1 

1. Realizei o pull da imagem `jupyter/all-spark-notebook`.
2. Criei um container interativo com o Spark e o Jupyter Lab.
3. Acessei o terminal do container com docker exec -it <container_id> /bin/bash.
4. Utilizei wget com token de autentica√ß√£o para baixar o arquivo README.md de um reposit√≥rio privado do GitHub.
5. Executei o PySpark no terminal com o comando pyspark.
6. Desenvolvi e executei os comandos em PySpark no terminal interativo para contar a ocorr√™ncia de cada palavra do arquivo README.md.

### C√≥digo Desenvolvido

O c√≥digo respons√°vel por realizar a contagem de palavras pode ser encontrado no seguinte arquivo:

 [`contador_palavras.pyspark`](./Exercicios/exercicio_1/codigo.py)
 [`Resultado`](./Exercicios/exercicio_1/resultado.txt)

## Exercicio 2

1. Fiz importa√ß√£o da bibliotecas necessarias
2. Configurei o projeto para carregar  chave da api sem exp√¥-la no c√≥digo
3. Consumi a API do TMDB para obter dados dos filmes mais bem avaliados.
4. Estruturei os dados em um DataFrame

O c√≥digo completo esta em 
[`notebook`](./Exercicios/exercicio_2/api.ipynb)
---
##  Evid√™ncias exercicio 1

###  Container Spark em execu√ß√£o
![Container em execu√ß√£o](./Exercicios/Imagens_Execucao/container_running.png)

---

###  Download do arquivo README.md com autentica√ß√£o
![Comando WGET com token](./Exercicios/Imagens_Execucao/WGET.png)

---

###  Inicializa√ß√£o do PySpark no container
![Inicializa√ß√£o do PySpark](./Exercicios/Imagens_Execucao/spark_iniciando.png)

---

###  Comandos PySpark para contagem de palavras
![Comandos PySpark](./Exercicios/Imagens_Execucao/comandos.png)

---

###  Resultado da contagem de palavras no README.md
![Resultado da contagem](./Exercicios/Imagens_Execucao/resultado.png)

---
##  Evid√™ncia exercicio 2

### Retorno da api
![Retorno da ape](./Exercicios/Imagens_Execucao/retorno_api.png)

---

### [Link para pasta de Imagens Execucao](./Exercicios/Imagens_Execucao)

##  Caminhos para as pastas da Sprint

- [ Certificados](./Certificados/)
- [ Desafio](./Desafio/)
- [ Evid√™ncias](./Evidencias/)
- [ Exerc√≠cios](./Exercicios/)


