# Resumo da Sprint 6 ‚Äì Continua√ß√£o de Analytics na AWS e Glue Getting Started

Nesta etapa da forma√ß√£o, avancei no dom√≠nio de solu√ß√µes anal√≠ticas serverless da AWS com foco em **governan√ßa de dados, arquitetura Lake House** e **pipelines ETL com AWS Glue**. Os cursos realizados nesta sprint fortaleceram minha capacidade de criar fluxos de dados escal√°veis e bem estruturados na nuvem.

---

## AWS Skill Builder: Fundamentals of Analytics on AWS ‚Äì Part 2 (Portugu√™s)

- Fundamentos de **data lakes** e **data warehouses**, seus benef√≠cios e arquiteturas.
- Cria√ß√£o de data lakes com **AWS Lake Formation**, com foco em governan√ßa e seguran√ßa.
- Vantagens do **Amazon Redshift** para data warehousing moderno (serverless, ETL zero, ML).
- Pilares da arquitetura de dados moderna: escalabilidade, acesso unificado, governan√ßa e custo-benef√≠cio.
- Conceitos de **data mesh**, **dados como produto** e uso do **Amazon DataZone**.
- Casos de uso reais de analytics por setor e arquiteturas de refer√™ncia com servi√ßos da AWS.

---

## AWS Glue Getting Started (Portugu√™s)

- Introdu√ß√£o ao **AWS Glue**, sua arquitetura e casos de uso.
- Cria√ß√£o de bucket no **Amazon S3** e carregamento de dados de amostra.
- Uso do **Glue Studio** para criar crawlers, catalogar dados e executar jobs ETL com PySpark.
- Uso do **Glue DataBrew** para carregar dados, criar perfis, projetos e f√≥rmulas de transforma√ß√£o.
- Demonstra√ß√£o pr√°tica de exclus√£o de recursos e boas pr√°ticas operacionais.

---

#  Desafio

- O arquivo desenvolvido e utilizado para a realiza√ß√£o do desafio desta sprint est√° dispon√≠vel na pasta Desafio, e a documenta√ß√£o completa pode ser consultada em seu respectivo `README.md`:
  - üìÇ [Pasta Desafio](./Desafio/)
  - üìÑ [README.md do Desafio](./Desafio/README.md)

# Exerc√≠cios

Nesta Sprint, realizei os exerc√≠cios **Gera√ß√£o e massa de dados**

Tambem realizei um  **Lab AWS GLUE**

##  Etapas Realizadas exercicio 1 - parte 1 

1. Fiz um script python, onde declarei e inicializei uma lista contendo 250 numeros interios obtidos de forma aleatoria.
2. Fiz um script python, onde declarei uma lista com nomes de 20 animais. Ordenei essa lista em ordem crescente e depois salvei a lista em um arquivo de texto
3. Elaborei um codigo python, para gerar um dataset de nomes de pessoas.

### C√≥digo Desenvolvido

O c√≥digo respons√°vel por realizar o exercicio **Gera√ß√£o e massa de dados** esta em:

 [`codigo`](./Exercicios/exercicio_1/parte_1.ipynb)
 [`Resultado_animais`](./Exercicios/exercicio_1/animais.txt)
 [`Resultado_nomes`](./Exercicios/exercicio_1/nomes_aleatorios.txt)

## Etapas Realizadas Exercicio 1 - parte 2 

1. Fiz a leitura do arquivo `nomes_aleatorios.txt` e fiz um print das 5 primeiras linhas.
2. Fiz o print do schema do arquivo e renomeei a coluna para `nomes`.
3. Adicionei uma coluna chamada `Escolaridade` e atribui 3 valores de forma aleatoria.
4. Adicionei uma nova coluna chamada `Pais` e atribui o nome de um dos 13 paises da Am√©rica do Sul, de forma aleat√≥ria.
5. Adicione uma nova coluna chamda `AnoNascimento` e atribui para cada linha um valor aleat√≥rio entre 1945 e 2010.
6. Usando o m√©todo SELECT, selecionei as pessoas que nasceram neste s√©culo e mostrei 10 nomes.
7. Fiz o mesmo processo da etapa 6 mas usando `Spark SQL`
8. Utilizei o m√©todo filter para ver o n√∫mero total de pessoas que s√£o da gera√ß√£o Millennials(1980 ate 1994)
9. Fiz o mesmo processo da etapa 8 mas com `Spark SQL`
10. Usando `Spark SQL` obtive a quantidade de pessoas de cada pais para cada uma das seguintes gera√ß√µes(`Baby Boomers`,`Gera√ß√£o X`,`Millennials`,`Gera√ß√£o Z`)

O c√≥digo completo esta em 
[`notebook`](./Exercicios/exercicio_2/)
---
##  Evid√™ncias exercicio 1

###  Container Spark em execu√ß√£o
![Container em execu√ß√£o](./Exercicios/Imagens_Execucao/container_running.png)

---

###  Download do arquivo README.md com autentica√ß√£o
![Comando WGET com token](./Exercicios/Imagens_Execucao/WGET.png)

---

###  Inicializa√ß√£o do PySpark no container
![Inicializa√ß√£o do PySpark](./Exercicios/Imagens_Execucao/spark_iniciando.png)

---

###  Comandos PySpark para contagem de palavras
![Comandos PySpark](./Exercicios/Imagens_Execucao/comandos.png)

---

###  Resultado da contagem de palavras no README.md
![Resultado da contagem](./Exercicios/Imagens_Execucao/resultado.png)

---
##  Evid√™ncia exercicio 2

### Retorno da api
![Retorno da ape](./Exercicios/Imagens_Execucao/retorno_api.png)

---

### [Link para pasta de Imagens Execucao](./Exercicios/Imagens_Execucao)

##  Caminhos para as pastas da Sprint

- [ Certificados](./Certificados/)
- [ Desafio](./Desafio/)
- [ Evid√™ncias](./Evidencias/)
- [ Exerc√≠cios](./Exercicios/)


